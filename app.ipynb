{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8883f822",
      "metadata": {},
      "source": [
        "# ðŸ§  Interview Support System: Fine-Tuned Falcon + Gemini"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07ac481d",
      "metadata": {},
      "source": [
        "## ðŸš€ Setup and Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "409aadd1",
      "metadata": {
        "id": "409aadd1"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "\n",
        "!pip install -qqq gradio --progress-bar off\n",
        "!pip install -qqq bitsandbytes==0.42.0 --progress-bar off\n",
        "!pip install -qqq torch==2.1.2 --progress-bar off\n",
        "!pip install -qqq -U transformers==4.39.3 --progress-bar off\n",
        "!pip install -qqq -U peft==0.10.0 --progress-bar off\n",
        "!pip install -qqq -U accelerate==0.29.3 --progress-bar off\n",
        "!pip install -qqq loralib==0.1.2 --progress-bar off\n",
        "!pip install -qqq einops==0.7.0 --progress-bar off\n",
        "!pip install -qqq google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0f79f94",
      "metadata": {},
      "source": [
        "## Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c465c8f",
      "metadata": {
        "id": "3c465c8f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "import requests\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de219b4",
      "metadata": {},
      "source": [
        "##  Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bbcd377",
      "metadata": {
        "id": "3bbcd377"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4c434ec",
      "metadata": {},
      "source": [
        "## Load Fine-Tuned Falcon Model (QLoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab6f70b6",
      "metadata": {
        "id": "ab6f70b6"
      },
      "outputs": [],
      "source": [
        "PEFT_MODEL = \"Pranav06/falcon-7b-qlora-interview_qa-support-bot\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    return_dict=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = PeftModel.from_pretrained(model, PEFT_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e343c341",
      "metadata": {},
      "source": [
        "## Generation Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vwFuE2hDATf4",
      "metadata": {
        "id": "vwFuE2hDATf4"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "DEVICE = \"cuda:0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "81457dba",
      "metadata": {
        "id": "81457dba"
      },
      "outputs": [],
      "source": [
        "generation_config = model.generation_config\n",
        "generation_config.max_new_tokens = 200\n",
        "generation_config.temperature = 0.7\n",
        "generation_config.top_p = 0.7\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d2ab4eb",
      "metadata": {},
      "source": [
        "## Falcon Prompting Function (Guidance Generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b6b276e5",
      "metadata": {
        "id": "b6b276e5"
      },
      "outputs": [],
      "source": [
        "def generate_response(question: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "<human>: {question}\n",
        "<assistant>:\n",
        "\"\"\".strip()\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            input_ids=encoding.input_ids,\n",
        "            attention_mask=encoding.attention_mask,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    assistant_start = \"<assistant>:\"\n",
        "    response_start = response.find(assistant_start)\n",
        "    response = response[response_start + len(assistant_start) :].strip()\n",
        "    response_lines = response.split(\"\\n\")\n",
        "    final_response = response_lines[0].strip()\n",
        "\n",
        "    return final_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1c13023f",
      "metadata": {
        "id": "1c13023f"
      },
      "outputs": [],
      "source": [
        "prompt = \"Why should we hire you?\"\n",
        "print(generate_response(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "579225de",
      "metadata": {},
      "source": [
        "## Setup Gemini API (for Final Interview Answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZkJs0n72Mifz",
      "metadata": {
        "id": "ZkJs0n72Mifz"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "gemini_generation_config = {\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 40,\n",
        "    \"max_output_tokens\": 8192,\n",
        "}\n",
        "\n",
        "gen_model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-2.0-flash-exp\",\n",
        "    generation_config=gemini_generation_config,\n",
        ")\n",
        "\n",
        "# Gemini Prompting Function\n",
        "def generate_gemini_response(guidance_text):\n",
        "  prompt = f\"\"\"\n",
        "    You are a professional job candidate preparing for an important interview.\n",
        "\n",
        "    Based on the following guidance: '{guidance_text}', write a realistic and professional interview answer:\n",
        "    - Use only first-person tone (\"I\", \"my\", \"me\").\n",
        "    - Do NOT include casual words like \"okay\", \"sure\", or \"here's\".\n",
        "    - Directly start the answer without any introduction.\n",
        "    - Keep it concise, confident, and include a real-world example or achievement if possible.\n",
        "    - Maintain a positive, proactive, and professional tone throughout.\n",
        "    \"\"\"\n",
        "\n",
        "  response = gen_model.generate_content(prompt)\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef971f5f",
      "metadata": {},
      "source": [
        "## Full Interview Assistant Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "rOV8heyrgc_y",
      "metadata": {
        "id": "rOV8heyrgc_y"
      },
      "outputs": [],
      "source": [
        "def interview_assistant(interview_question):\n",
        "    # Step 1: Your Fine-tuned Falcon Model gives Guidance\n",
        "    guidance = generate_response(interview_question)\n",
        "\n",
        "    # Step 2: Send Guidance to Gemini and get Final Example Answer\n",
        "    example_answer = generate_gemini_response(guidance)\n",
        "\n",
        "    return guidance, example_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f584eba",
      "metadata": {},
      "source": [
        "## Gradio Web App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iB-o8b_kgfWp",
      "metadata": {
        "id": "iB-o8b_kgfWp"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"<h1 style='text-align: center;'>ðŸ§  Interview Support System</h1>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        question_input = gr.Textbox(placeholder=\"Enter an interview question...\", label=\"Interview Question\")\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_btn = gr.Button(\"Generate Answer\")\n",
        "\n",
        "    with gr.Row():\n",
        "        guidance_output = gr.Textbox(label=\"Model Guidance\", interactive=False)\n",
        "        example_output = gr.Textbox(label=\"Example Interview Answer\", interactive=False)\n",
        "\n",
        "    submit_btn.click(\n",
        "        interview_assistant,\n",
        "        inputs=[question_input],\n",
        "        outputs=[guidance_output, example_output]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
